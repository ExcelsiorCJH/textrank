{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE Score V01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import rouge\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from textrank import TextRank\n",
    "\n",
    "from utils import pdf_to_text\n",
    "from types_ import *\n",
    "\n",
    "# ignore warning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../data/en/raw\"\n",
    "file_list = glob(f\"{dir_path}/*.pdf\")\n",
    "\n",
    "fnames = []\n",
    "for fname in file_list:\n",
    "    _, fname = os.path.split(fname)\n",
    "    fname, _ = os.path.splitext(fname)\n",
    "    fnames.append(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_path = '../data/en/global'\n",
    "ref_list = glob(f'{ref_path}/*.txt')\n",
    "\n",
    "ref_names = []\n",
    "for ref in ref_list:\n",
    "    _, ref = os.path.split(ref)\n",
    "    ref, _ = os.path.splitext(ref)\n",
    "    ref_names.append(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fnames = []\n",
    "for ref_name in ref_names:\n",
    "    if ref_name in fnames:\n",
    "        eval_fnames.append(ref_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "textrank = TextRank(language=\"en\", tokenizer=None, stopwords=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_path = '../data/en/hyp'\n",
    "ref_path = '../data/en/ref'\n",
    "\n",
    "if not os.path.exists(hyp_path):\n",
    "    os.mkdir(hyp_path)\n",
    "if not os.path.exists(ref_path):\n",
    "    os.mkdir(ref_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 57/57 [01:54<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/en/raw'\n",
    "\n",
    "file_id = 1\n",
    "for fname in tqdm(eval_fnames):\n",
    "    abs_fname = f'{file_path}/{fname}.pdf'\n",
    "    \n",
    "    sents = pdf_to_text(abs_fname)\n",
    "    keysents = textrank.summarize(sents, topk=3)\n",
    "    \n",
    "    with open(f\"{hyp_path}/{fname}.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(\"\\n\".join(keysents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RougeScorer:\n",
    "    def __init__(self, use_tokenizer=True):\n",
    "\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        if use_tokenizer:\n",
    "            self.tokenizer = Mecab()\n",
    "\n",
    "        self.rouge_evaluator = rouge.Rouge(\n",
    "            metrics=[\"rouge-n\", \"rouge-l\"],\n",
    "            max_n=2,\n",
    "            limit_length=True,\n",
    "            length_limit=1000,\n",
    "            length_limit_type=\"words\",\n",
    "            apply_avg=True,\n",
    "            apply_best=False,\n",
    "            alpha=0.5,  # Default F1_score\n",
    "            weight_factor=1.2,\n",
    "            stemming=True,\n",
    "        )\n",
    "\n",
    "    def compute_rouge(self, ref_path, hyp_path):\n",
    "        ref_fnames = glob(f\"{ref_path}/*.txt\")\n",
    "        hyp_fnames = glob(f\"{hyp_path}/*.txt\")\n",
    "        ref_fnames.sort()\n",
    "        hyp_fnames.sort()\n",
    "\n",
    "        self.reference_summaries = []\n",
    "        self.generated_summaries = []\n",
    "\n",
    "        for ref_fname, hyp_fname in tqdm(\n",
    "            zip(ref_fnames, hyp_fnames), total=len(ref_fnames)\n",
    "        ):\n",
    "            assert os.path.split(ref_fname)[1] == os.path.split(hyp_fname)[1]\n",
    "\n",
    "            with open(ref_fname, \"r\", encoding=\"utf8\") as f:\n",
    "                ref = f.read().split(\"\\n\")\n",
    "                ref = \"\".join(ref)\n",
    "\n",
    "            with open(hyp_fname, \"r\", encoding=\"utf8\") as f:\n",
    "                hyp = f.read().split(\"\\n\")\n",
    "                hyp = \"\".join(hyp)\n",
    "\n",
    "            if self.use_tokenizer:\n",
    "                ref = self.tokenizer.morphs(ref)\n",
    "                hyp = self.tokenizer.morphs(hyp)\n",
    "\n",
    "            ref = \" \".join(ref)\n",
    "            hyp = \" \".join(hyp)\n",
    "\n",
    "            self.reference_summaries.append(ref)\n",
    "            self.generated_summaries.append(hyp)\n",
    "\n",
    "        scores = self.rouge_evaluator.get_scores(\n",
    "            self.generated_summaries, self.reference_summaries\n",
    "        )\n",
    "        str_scores = self.format_rouge_scores(scores)\n",
    "        self.save_rouge_scores(str_scores)\n",
    "        return str_scores\n",
    "\n",
    "    def save_rouge_scores(self, str_scores):\n",
    "        with open(\"rouge_scores.txt\", \"w\") as output:\n",
    "            output.write(str_scores)\n",
    "\n",
    "    def format_rouge_scores(self, scores):\n",
    "        return \"\"\"\\n\n",
    "    ****** ROUGE SCORES ******\n",
    "    ** ROUGE 1\n",
    "    F1        >> {:.3f}\n",
    "    Precision >> {:.3f}\n",
    "    Recall    >> {:.3f}\n",
    "    ** ROUGE 2\n",
    "    F1        >> {:.3f}\n",
    "    Precision >> {:.3f}\n",
    "    Recall    >> {:.3f}\n",
    "    ** ROUGE L\n",
    "    F1        >> {:.3f}\n",
    "    Precision >> {:.3f}\n",
    "    Recall    >> {:.3f}\"\"\".format(\n",
    "            scores[\"rouge-1\"][\"f\"],\n",
    "            scores[\"rouge-1\"][\"p\"],\n",
    "            scores[\"rouge-1\"][\"r\"],\n",
    "            scores[\"rouge-2\"][\"f\"],\n",
    "            scores[\"rouge-2\"][\"p\"],\n",
    "            scores[\"rouge-2\"][\"r\"],\n",
    "            scores[\"rouge-l\"][\"f\"],\n",
    "            scores[\"rouge-l\"][\"p\"],\n",
    "            scores[\"rouge-l\"][\"r\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_eval = RougeScorer(use_tokenizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 57/57 [00:00<00:00, 84.44it/s]\n"
     ]
    }
   ],
   "source": [
    "ref_path = \"../data/en/global/\"\n",
    "hyp_path = \"../data/en/hyp\"\n",
    "\n",
    "result = rouge_eval.compute_rouge(ref_path, hyp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    ****** ROUGE SCORES ******\n",
      "    ** ROUGE 1\n",
      "    F1        >> 0.742\n",
      "    Precision >> 0.684\n",
      "    Recall    >> 0.913\n",
      "    ** ROUGE 2\n",
      "    F1        >> 0.605\n",
      "    Precision >> 0.558\n",
      "    Recall    >> 0.746\n",
      "    ** ROUGE L\n",
      "    F1        >> 0.524\n",
      "    Precision >> 0.485\n",
      "    Recall    >> 0.622\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tf-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
