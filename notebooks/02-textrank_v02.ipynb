{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank - ver02 using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# black formatting\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# tokenizer import\n",
    "from konlpy.tag import Okt, Komoran, Hannanum, Kkma\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        from eunjeon import Mecab\n",
    "    except:\n",
    "        print(\"please install eunjeon module\")\n",
    "else:  # Ubuntu일 경우\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "from types_ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Common "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sents.txt\", \"r\") as f:\n",
    "    sents = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) get tokenizer & get tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_name):\n",
    "    if tokenizer_name == \"komoran\":\n",
    "        tokenizer = Komoran()\n",
    "    elif tokenizer_name == \"okt\":\n",
    "        tokenizer = Okt()\n",
    "    elif tokenizer_name == \"mecab\":\n",
    "        tokenizer = Mecab()\n",
    "    elif tokenizer_name == \"hannanum\":\n",
    "        tokenizer = Hannanum()\n",
    "    elif tokenizer_name == \"kkma\":\n",
    "        tokenizer = Kkma()\n",
    "    else:\n",
    "        tokenizer = Mecab()\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = get_tokenizer(\"mecab\")\n",
    "# tokenizer.pos(\"아버지가방에들어가신다_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 각 tokenizer 별 명사, 형용사, 동사, 어근\n",
    "# komoran_pos = ['/NN', '/XR', '/VA', '/VV']\n",
    "# okt_pos = ['/Noun', '/Verb', '/Adjective']\n",
    "# mecab_pos = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sent: List[str], noun=False, tokenizer=\"mecab\") -> List[str]:\n",
    "    tokenizer = get_tokenizer(tokenizer)\n",
    "\n",
    "    if noun:\n",
    "        return tokenizer.nouns(sent)\n",
    "\n",
    "    return tokenizer.morphs(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_tokens(sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Vectorize Sents using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"연합뉴스\", \"가방\"]\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=stopwords,\n",
    "    tokenizer=partial(get_tokens, noun=False, tokenizer=\"mecab\"),\n",
    "    min_df=2,\n",
    ")\n",
    "\n",
    "x = vectorizer.fit_transform(sents)\n",
    "\n",
    "# vectorizer.get_feature_names()\n",
    "\n",
    "x.toarray().shape\n",
    "\n",
    "vocab_idx = vectorizer.vocabulary_\n",
    "idx_vocab = {idx: vocab for vocab, idx in vocab_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sents(\n",
    "    sents: List[str], stopwords=None, min_count=2, tokenizer=\"mecab\", noun=False\n",
    "):\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=stopwords,\n",
    "        tokenizer=partial(get_tokens, noun=False, tokenizer=\"mecab\"),\n",
    "        min_df=min_count,\n",
    "    )\n",
    "\n",
    "    vec = vectorizer.fit_transform(sents)\n",
    "    vocab_idx = vectorizer.vocabulary_\n",
    "    idx_vocab = {idx: vocab for vocab, idx in vocab_idx.items()}\n",
    "    return vec, vocab_idx, idx_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"연합뉴스\", \"가방\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, vocab_idx, idx_vocab = vectorize_sents(sents, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary csr_matrix\n",
    "numerators = (x > 0) * 1\n",
    "\n",
    "# Inverse sentence length\n",
    "min_length = 1\n",
    "denominators = np.asarray(x.sum(axis=1))\n",
    "denominators[np.where(denominators <= min_length)] = 10000\n",
    "denominators = np.log(denominators)\n",
    "\n",
    "denom_log1 = np.matmul(denominators, np.ones(denominators.shape).T)\n",
    "denom_log2 = np.matmul(np.ones(denominators.shape), denominators.T)\n",
    "\n",
    "sim_mat = np.dot(numerators, numerators.T)\n",
    "\n",
    "sim_mat = sim_mat / (denom_log1 + denom_log2)\n",
    "\n",
    "min_sim = 0.3\n",
    "sim_mat[np.where(sim_mat <= min_sim)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(x, min_sim=0.3, min_length=1):\n",
    "    \"\"\"\n",
    "    $$\n",
    "    sim(s_1, s_2) = \n",
    "    \\frac{\\vert \\{ w_k \\vert w_k \\in S_1 \\& w_k \\in S_2 \\} \\vert}\n",
    "    {log \\vert S_1 \\vert + log \\vert S_2 \\vert}\n",
    "    $$\n",
    "    \"\"\"\n",
    "\n",
    "    # binary csr_matrix\n",
    "    numerators = (x > 0) * 1\n",
    "\n",
    "    # denominator\n",
    "    min_length = 1\n",
    "    denominators = np.asarray(x.sum(axis=1))\n",
    "    denominators[np.where(denominators <= min_length)] = 10000\n",
    "    denominators = np.log(denominators)\n",
    "    denom_log1 = np.matmul(denominators, np.ones(denominators.shape).T)\n",
    "    denom_log2 = np.matmul(np.ones(denominators.shape), denominators.T)\n",
    "\n",
    "    sim_mat = np.dot(numerators, numerators.T)\n",
    "    sim_mat = sim_mat / (denom_log1 + denom_log2)\n",
    "    sim_mat[np.where(sim_mat <= min_sim)] = 0\n",
    "\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = similarity_matrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat = csr_matrix(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(x, min_sim=0.3):\n",
    "    sim_mat = 1 - pairwise_distances(x, metric=\"cosine\")\n",
    "    sim_mat[np.where(sim_mat <= min_sim)] = 0\n",
    "\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = cosine_similarity_matrix(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Sentence Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sents.txt\", \"r\") as f:\n",
    "    sents = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"연합뉴스\", \"가방\"]\n",
    "min_count = 2\n",
    "tokenizer = \"mecab\"\n",
    "\n",
    "vecs, vocab_idx, idx_vocab = vectorize_sents(\n",
    "    sents, stopwords, min_count=min_count, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = \"cosine\"\n",
    "min_sim = 0.3\n",
    "\n",
    "if similarity == \"cosine\":\n",
    "    vecs = cosine_similarity_matrix(vecs, min_sim=min_sim)\n",
    "else:\n",
    "    vecs = similarity_matrix(vecs, min_sim=min_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_graph(\n",
    "    sents: List[str],\n",
    "    min_count=2,\n",
    "    min_sim=0.3,\n",
    "    tokenizer=\"mecab\",\n",
    "    noun=False,\n",
    "    similarity=None,\n",
    "    stopwords: List[str] = [\"뉴스\", \"그리고\"],\n",
    "):\n",
    "\n",
    "    mat, vocab_idx, idx_vocab = vectorize_sents(\n",
    "        sents, stopwords, min_count=min_count, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    if similarity == \"cosine\":\n",
    "        mat = cosine_similarity_matrix(mat, min_sim=min_sim)\n",
    "    else:\n",
    "        mat = similarity_matrix(mat, min_sim=min_sim)\n",
    "\n",
    "    return mat, vocab_idx, idx_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"연합뉴스\", \"가방\"]\n",
    "\n",
    "mat, vocab_idx, idx_vocab = sent_graph(sents, stopwords=stopwords, similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sknetwork.ranking import PageRank\n",
    "\n",
    "# pr = PageRank()\n",
    "\n",
    "# scores = pr.fit_transform(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06486137, 0.03777102, 0.06637652, 0.06136609, 0.04007189,\n",
       "       0.055716  , 0.04917619, 0.05037918, 0.06195448, 0.05159843,\n",
       "       0.04580437, 0.05314427, 0.04182777, 0.03399724, 0.04825147,\n",
       "       0.04439981, 0.05794319, 0.03137895, 0.06365065, 0.04033112])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 0.85\n",
    "max_iter = 50\n",
    "method = \"iterative\"\n",
    "\n",
    "assert 0 < df < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = normalize(mat, axis=0, norm=\"l1\")\n",
    "R = np.ones(A.shape[0])\n",
    "N = np.ones(R.shape[0]) / R.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration\n",
    "for _ in range(max_iter):\n",
    "    R = df * np.matmul(A, R) + (1 - df) * N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"algebraic\"\n",
    "\n",
    "A = normalize(mat, axis=0, norm=\"l1\")\n",
    "I = np.eye(A.shape[0])\n",
    "N = np.ones(A.shape[0]) / A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.linalg.inv((I - df * A))\n",
    "R = np.matmul(R, (1 - df) * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(x: np.ndarray, df=0.85, max_iter=50, method=\"iterative\"):\n",
    "    \"\"\"\n",
    "    PageRank function\n",
    "    ==================\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x : np.ndarray\n",
    "    df : float\n",
    "        Damping Factor, 0 < df < 1\n",
    "    max_iter : int\n",
    "        Maximum number of iteration for Power method\n",
    "    method : str\n",
    "        default is iterative, oter algebraic\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    R : np.ndarray\n",
    "        PageRank vector (score)\n",
    "    \"\"\"\n",
    "\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    A = normalize(mat, axis=0, norm=\"l1\")\n",
    "    N = np.ones(A.shape[0]) / A.shape[0]\n",
    "\n",
    "    if method == \"iterative\":\n",
    "        R = np.ones(A.shape[0])\n",
    "        # iteration\n",
    "        for _ in range(max_iter):\n",
    "            R = df * np.matmul(A, R) + (1 - df) * N\n",
    "    elif method == \"algebraic\":\n",
    "        R = np.linalg.inv((I - df * A))\n",
    "        R = np.matmul(R, (1 - df) * N)\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = pagerank(mat, method=\"iterative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 3\n",
    "idxs = R.argsort()[-topk:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "keysents = [(idx, R[idx], sents[idx]) for idx in sorted(idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  0.06523941197137159,\n",
       "  '오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다'),\n",
       " (2,\n",
       "  0.06676676734182561,\n",
       "  '경찰에 따르면 성씨는 19일 오후 강북경찰서 인근 부동산 업소 밖에서 부동산업자 이모 67 씨가 나오기를 기다렸다 이씨와는 평소에도 말다툼을 자주 한 것으로 알려졌다'),\n",
       " (18,\n",
       "  0.06401999523713223,\n",
       "  '경찰은 인근을 수색해 성씨가 만든 사제총 16정과 칼 7개를 압수했다 실제 폭발할지는 알 수 없는 요구르트병에 무언가를 채워두고 심지를 꽂은 사제 폭탄도 발견됐다')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18,  0,  2])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
