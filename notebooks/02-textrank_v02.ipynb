{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank - ver02 using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# black formatting\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# tokenizer import\n",
    "from konlpy.tag import Okt, Komoran, Hannanum, Kkma\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        from eunjeon import Mecab\n",
    "    except:\n",
    "        print(\"please install eunjeon module\")\n",
    "else:  # Ubuntu일 경우\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "from types_ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Common "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sents.txt\", \"r\") as f:\n",
    "    sents = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) get tokenizer & get tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_name):\n",
    "    if tokenizer_name == \"komoran\":\n",
    "        tokenizer = Komoran()\n",
    "    elif tokenizer_name == \"okt\":\n",
    "        tokenizer = Okt()\n",
    "    elif tokenizer_name == \"mecab\":\n",
    "        tokenizer = Mecab()\n",
    "    elif tokenizer_name == \"hannanum\":\n",
    "        tokenizer = Hannanum()\n",
    "    elif tokenizer_name == \"kkma\":\n",
    "        tokenizer = Kkma()\n",
    "    else:\n",
    "        tokenizer = Mecab()\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = get_tokenizer(\"mecab\")\n",
    "# tokenizer.pos(\"아버지가방에들어가신다_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 각 tokenizer 별 명사, 형용사, 동사, 어근\n",
    "# komoran_pos = ['/NN', '/XR', '/VA', '/VV']\n",
    "# okt_pos = ['/Noun', '/Verb', '/Adjective']\n",
    "# mecab_pos = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sent: List[str], noun=False, tokenizer=\"mecab\") -> List[str]:\n",
    "    tokenizer = get_tokenizer(tokenizer)\n",
    "\n",
    "    if noun:\n",
    "        return tokenizer.nouns(sent)\n",
    "\n",
    "    return tokenizer.morphs(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_tokens(sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Vectorize Sents using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"연합뉴스\", \"가방\"]\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=stopwords,\n",
    "    tokenizer=partial(get_tokens, noun=False, tokenizer=\"mecab\"),\n",
    "    min_df=2,\n",
    ")\n",
    "\n",
    "x = vectorizer.fit_transform(sents)\n",
    "\n",
    "# vectorizer.get_feature_names()\n",
    "\n",
    "x.toarray().shape\n",
    "\n",
    "vocab_idx = vectorizer.vocabulary_\n",
    "idx_vocab = {idx: vocab for vocab, idx in vocab_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sents(\n",
    "    sents: List[str], stopwords=None, min_count=2, tokenizer=\"mecab\", noun=False\n",
    "):\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=stopwords,\n",
    "        tokenizer=partial(get_tokens, noun=False, tokenizer=\"mecab\"),\n",
    "        min_df=min_count,\n",
    "    )\n",
    "\n",
    "    vec = vectorizer.fit_transform(sents)\n",
    "    vocab_idx = vectorizer.vocabulary_\n",
    "    idx_vocab = {idx: vocab for vocab, idx in vocab_idx.items()}\n",
    "    return vec, vocab_idx, idx_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"연합뉴스\", \"가방\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, vocab_idx, idx_vocab = vectorize_sents(sents, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary csr_matrix\n",
    "numerators = (x > 0) * 1\n",
    "\n",
    "# Inverse sentence length\n",
    "min_length = 1\n",
    "denominators = np.asarray(x.sum(axis=1))\n",
    "denominators[np.where(denominators <= min_length)] = 10000\n",
    "denominators = np.log(denominators)\n",
    "\n",
    "denom_log1 = np.matmul(denominators, np.ones(denominators.shape).T)\n",
    "denom_log2 = np.matmul(np.ones(denominators.shape), denominators.T)\n",
    "\n",
    "sim_mat = np.dot(numerators, numerators.T)\n",
    "\n",
    "sim_mat = sim_mat / (denom_log1 + denom_log2)\n",
    "\n",
    "min_sim = 0.3\n",
    "sim_mat[np.where(sim_mat <= min_sim)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(x, min_sim=0.3, min_length=1):\n",
    "    \"\"\"\n",
    "    $$\n",
    "    sim(s_1, s_2) = \n",
    "    \\frac{\\vert \\{ w_k \\vert w_k \\in S_1 \\& w_k \\in S_2 \\} \\vert}\n",
    "    {log \\vert S_1 \\vert + log \\vert S_2 \\vert}\n",
    "    $$\n",
    "    \"\"\"\n",
    "\n",
    "    # binary csr_matrix\n",
    "    numerators = (x > 0) * 1\n",
    "\n",
    "    # denominator\n",
    "    min_length = 1\n",
    "    denominators = np.asarray(x.sum(axis=1))\n",
    "    denominators[np.where(denominators <= min_length)] = 10000\n",
    "    denominators = np.log(denominators)\n",
    "    denom_log1 = np.matmul(denominators, np.ones(denominators.shape).T)\n",
    "    denom_log2 = np.matmul(np.ones(denominators.shape), denominators.T)\n",
    "\n",
    "    sim_mat = np.dot(numerators, numerators.T)\n",
    "    sim_mat = sim_mat / (denom_log1 + denom_log2)\n",
    "    sim_mat[np.where(sim_mat <= min_sim)] = 0\n",
    "\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = similarity_matrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat = csr_matrix(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(x, min_sim=0.3):\n",
    "    sim_mat = 1 - pairwise_distances(x, metric=\"cosine\")\n",
    "    sim_mat[np.where(sim_mat <= min_sim)] = 0\n",
    "\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = cosine_similarity_matrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
