{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank - ver02 using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# black formatting\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# tokenizer import\n",
    "from konlpy.tag import Okt, Komoran, Hannanum, Kkma\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        from eunjeon import Mecab\n",
    "    except:\n",
    "        print(\"please install eunjeon module\")\n",
    "else:  # Ubuntu일 경우\n",
    "    from konlpy.tag import Mecab\n",
    "\n",
    "from types_ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Common "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sents.txt\", \"r\") as f:\n",
    "    sents = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) get tokenizer & get tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(tokenizer_name):\n",
    "    if tokenizer_name == \"komoran\":\n",
    "        tokenizer = Komoran()\n",
    "    elif tokenizer_name == \"okt\":\n",
    "        tokenizer = Okt()\n",
    "    elif tokenizer_name == \"mecab\":\n",
    "        tokenizer = Mecab()\n",
    "    elif tokenizer_name == \"hannanum\":\n",
    "        tokenizer = Hannanum()\n",
    "    elif tokenizer_name == \"kkma\":\n",
    "        tokenizer = Kkma()\n",
    "    else:\n",
    "        tokenizer = Mecab()\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = get_tokenizer(\"mecab\")\n",
    "# tokenizer.pos(\"아버지가방에들어가신다_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 각 tokenizer 별 명사, 형용사, 동사, 어근\n",
    "# komoran_pos = ['/NN', '/XR', '/VA', '/VV']\n",
    "# okt_pos = ['/Noun', '/Verb', '/Adjective']\n",
    "# mecab_pos = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sent: List[str], noun=False, tokenizer=\"mecab\") -> List[str]:\n",
    "    tokenizer = get_tokenizer(tokenizer)\n",
    "\n",
    "    if noun:\n",
    "        nouns = tokenizer.nouns(sent)\n",
    "        nouns = [word for word in nouns if len(word) > 1]\n",
    "        return nouns\n",
    "\n",
    "    return tokenizer.morphs(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_tokens(sents[0], noun=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Vectorize Sents using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"연합뉴스\", \"가방\"]\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=stopwords,\n",
    "    tokenizer=partial(get_tokens, noun=False, tokenizer=\"mecab\"),\n",
    "    min_df=2,\n",
    ")\n",
    "\n",
    "x = vectorizer.fit_transform(sents)\n",
    "\n",
    "# vectorizer.get_feature_names()\n",
    "\n",
    "x.toarray().shape\n",
    "\n",
    "vocab_idx = vectorizer.vocabulary_\n",
    "idx_vocab = {idx: vocab for vocab, idx in vocab_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sents(\n",
    "    sents: List[str], stopwords=None, min_count=2, tokenizer=\"mecab\", noun=False\n",
    "):\n",
    "\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=stopwords,\n",
    "        tokenizer=partial(get_tokens, noun=noun, tokenizer=\"mecab\"),\n",
    "        min_df=min_count,\n",
    "    )\n",
    "\n",
    "    vec = vectorizer.fit_transform(sents)\n",
    "    vocab_idx = vectorizer.vocabulary_\n",
    "    idx_vocab = {idx: vocab for vocab, idx in vocab_idx.items()}\n",
    "    return vec, vocab_idx, idx_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"연합뉴스\", \"가방\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, vocab_idx, idx_vocab = vectorize_sents(sents, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary csr_matrix\n",
    "numerators = (x > 0) * 1\n",
    "\n",
    "# Inverse sentence length\n",
    "min_length = 1\n",
    "denominators = np.asarray(x.sum(axis=1))\n",
    "denominators[np.where(denominators <= min_length)] = 10000\n",
    "denominators = np.log(denominators)\n",
    "\n",
    "denom_log1 = np.matmul(denominators, np.ones(denominators.shape).T)\n",
    "denom_log2 = np.matmul(np.ones(denominators.shape), denominators.T)\n",
    "\n",
    "sim_mat = np.dot(numerators, numerators.T)\n",
    "\n",
    "sim_mat = sim_mat / (denom_log1 + denom_log2)\n",
    "\n",
    "min_sim = 0.3\n",
    "sim_mat[np.where(sim_mat <= min_sim)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(x, min_sim=0.3, min_length=1):\n",
    "    \"\"\"\n",
    "    $$\n",
    "    sim(s_1, s_2) = \n",
    "    \\frac{\\vert \\{ w_k \\vert w_k \\in S_1 \\& w_k \\in S_2 \\} \\vert}\n",
    "    {log \\vert S_1 \\vert + log \\vert S_2 \\vert}\n",
    "    $$\n",
    "    \"\"\"\n",
    "\n",
    "    # binary csr_matrix\n",
    "    numerators = (x > 0) * 1\n",
    "\n",
    "    # denominator\n",
    "    min_length = 1\n",
    "    denominators = np.asarray(x.sum(axis=1))\n",
    "    denominators[np.where(denominators <= min_length)] = 10000\n",
    "    denominators = np.log(denominators)\n",
    "    denom_log1 = np.matmul(denominators, np.ones(denominators.shape).T)\n",
    "    denom_log2 = np.matmul(np.ones(denominators.shape), denominators.T)\n",
    "\n",
    "    sim_mat = np.dot(numerators, numerators.T)\n",
    "    sim_mat = sim_mat / (denom_log1 + denom_log2)\n",
    "    sim_mat[np.where(sim_mat <= min_sim)] = 0\n",
    "\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = similarity_matrix(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat = csr_matrix(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(x, min_sim=0.3):\n",
    "    sim_mat = 1 - pairwise_distances(x, metric=\"cosine\")\n",
    "    sim_mat[np.where(sim_mat <= min_sim)] = 0\n",
    "\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = cosine_similarity_matrix(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Sentence Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sents.txt\", \"r\") as f:\n",
    "    sents = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"연합뉴스\", \"가방\"]\n",
    "min_count = 2\n",
    "tokenizer = \"mecab\"\n",
    "\n",
    "vecs, vocab_idx, idx_vocab = vectorize_sents(\n",
    "    sents, stopwords, min_count=min_count, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = \"cosine\"\n",
    "min_sim = 0.3\n",
    "\n",
    "if similarity == \"cosine\":\n",
    "    vecs = cosine_similarity_matrix(vecs, min_sim=min_sim)\n",
    "else:\n",
    "    vecs = similarity_matrix(vecs, min_sim=min_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_graph(\n",
    "    sents: List[str],\n",
    "    min_count=2,\n",
    "    min_sim=0.3,\n",
    "    tokenizer=\"mecab\",\n",
    "    noun=False,\n",
    "    similarity=None,\n",
    "    stopwords: List[str] = [\"뉴스\", \"그리고\"],\n",
    "):\n",
    "\n",
    "    mat, vocab_idx, idx_vocab = vectorize_sents(\n",
    "        sents, stopwords, min_count=min_count, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    if similarity == \"cosine\":\n",
    "        mat = cosine_similarity_matrix(mat, min_sim=min_sim)\n",
    "    else:\n",
    "        mat = similarity_matrix(mat, min_sim=min_sim)\n",
    "\n",
    "    return mat, vocab_idx, idx_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"연합뉴스\", \"가방\"]\n",
    "\n",
    "mat, vocab_idx, idx_vocab = sent_graph(sents, stopwords=stopwords, similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity_matrix(x, min_sim=0.3):\n",
    "    sim_mat = 1 - pairwise_distances(x.T, metric=\"cosine\")\n",
    "    sim_mat[np.where(sim_mat <= min_sim)] = 0\n",
    "\n",
    "    return sim_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = word_similarity_matrix(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Word Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_graph(\n",
    "    sents: List[str],\n",
    "    min_count=2,\n",
    "    min_sim=0.3,\n",
    "    tokenizer=\"mecab\",\n",
    "    noun=True,\n",
    "    stopwords: List[str] = [\"연합뉴스\", \"그리고\", \"기자\"],\n",
    "):\n",
    "\n",
    "    mat, vocab_idx, idx_vocab = vectorize_sents(\n",
    "        sents, stopwords, min_count=min_count, tokenizer=tokenizer, noun=noun\n",
    "    )\n",
    "\n",
    "    mat = word_similarity_matrix(mat, min_sim=min_sim)\n",
    "\n",
    "    return mat, vocab_idx, idx_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat, vocab_idx, idx_vocab = word_graph(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sknetwork.ranking import PageRank\n",
    "# pr = PageRank()\n",
    "# scores = pr.fit_transform(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 0.85\n",
    "max_iter = 50\n",
    "method = \"iterative\"\n",
    "\n",
    "assert 0 < df < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = normalize(mat, axis=0, norm=\"l1\")\n",
    "R = np.ones(A.shape[0])\n",
    "N = np.ones(R.shape[0]) / R.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration\n",
    "for _ in range(max_iter):\n",
    "    R = df * np.matmul(A, R) + (1 - df) * N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"algebraic\"\n",
    "\n",
    "A = normalize(mat, axis=0, norm=\"l1\")\n",
    "I = np.eye(A.shape[0])\n",
    "N = np.ones(A.shape[0]) / A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.linalg.inv((I - df * A))\n",
    "R = np.matmul(R, (1 - df) * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(x: np.ndarray, df=0.85, max_iter=50, method=\"iterative\"):\n",
    "    \"\"\"\n",
    "    PageRank function\n",
    "    ==================\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x : np.ndarray\n",
    "    df : float\n",
    "        Damping Factor, 0 < df < 1\n",
    "    max_iter : int\n",
    "        Maximum number of iteration for Power method\n",
    "    method : str\n",
    "        default is iterative, oter algebraic\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    R : np.ndarray\n",
    "        PageRank vector (score)\n",
    "    \"\"\"\n",
    "\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    A = normalize(mat, axis=0, norm=\"l1\")\n",
    "    N = np.ones(A.shape[0]) / A.shape[0]\n",
    "\n",
    "    if method == \"iterative\":\n",
    "        R = np.ones(A.shape[0])\n",
    "        # iteration\n",
    "        for _ in range(max_iter):\n",
    "            R = df * np.matmul(A, R) + (1 - df) * N\n",
    "    elif method == \"algebraic\":\n",
    "        R = np.linalg.inv((I - df * A))\n",
    "        R = np.matmul(R, (1 - df) * N)\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = pagerank(mat, method=\"iterative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Sentence & Word Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 10\n",
    "idxs = R.argsort()[-topk:]\n",
    "\n",
    "# keysents = [(idx, R[idx], sents[idx]) for idx in sorted(idxs)]\n",
    "# keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [(idx, R[idx], idx_vocab[idx]) for idx in reversed(idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19, 0.038754667200304176, '서울'),\n",
       " (27, 0.03861352663379311, '용의자'),\n",
       " (9, 0.03661624569236274, '발사'),\n",
       " (16, 0.03546070933569919, '사제'),\n",
       " (2, 0.03517313567434101, '강북구'),\n",
       " (5, 0.03478439367464611, '경찰관'),\n",
       " (32, 0.03478439367464611, '조사'),\n",
       " (33, 0.03410792629851021, '총기'),\n",
       " (38, 0.0335252351049594, '폭행'),\n",
       " (39, 0.033476384490609516, '현장')]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
